{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import random\n",
    "random.seed(530)\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Input, Concatenate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Volumes/T7/Ubiosis/total_data.csv\"\n",
    "data_df = pd.read_csv(data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "data_df.dropna(axis=0,inplace=True)\n",
    "data_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정보 별 데이터프레임 분할\n",
    "radius_df = data_df.iloc[:,:1]\n",
    "cis1_df = data_df.iloc[:,1:6001]\n",
    "cis2_df = data_df.iloc[:,6001:12001]\n",
    "shear_df = data_df.iloc[:,12001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "ohe_cols = []\n",
    "for i in range(0,19):\n",
    "    ohe_cols.append(((310+i)/100))\n",
    "\n",
    "ohe_target = np.array(ohe_cols).reshape(-1,1)\n",
    "ohe_value = np.array(radius_df[\"RADIUS\"]).reshape(-1,1)\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(ohe_target)\n",
    "\n",
    "ohe_labels = ohe.transform(ohe_value)\n",
    "ohe_targets = ohe_labels.toarray()\n",
    "\n",
    "ohe_df = pd.DataFrame(columns=ohe_cols,data=ohe_targets)\n",
    "ohe_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shear Rate Scaling\n",
    "scale_list = [10,10,10,10,10,10,10,15,20]\n",
    "shear_df = shear_df.div(scale_list, axis=1)\n",
    "\n",
    "shear_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_cols = [\"1\",\"2\",\"5\",\"10\",\"50\",\"100\",\"150\",\"300\",\"1000\"]\n",
    "re_shear_df = shear_df[re_cols]\n",
    "re_shear_df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([radius_df, cis1_df, cis2_df, re_shear_df],axis=1)\n",
    "data.tail(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=530)\n",
    "\n",
    "X_train = train.iloc[:,:-9].reset_index(drop=True)\n",
    "y_train = train.iloc[:,-9:].reset_index(drop=True)\n",
    "X_test = test.iloc[:,:-9].reset_index(drop=True)\n",
    "y_test = test.iloc[:,-9:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정보 별 데이터프레임 분할\n",
    "tr_radius_df = X_train.iloc[:,:1]\n",
    "tr_cis1_df = X_train.iloc[:,1:6001]\n",
    "tr_cis2_df = X_train.iloc[:,6001:]\n",
    "\n",
    "te_radius_df = X_test.iloc[:,:1]\n",
    "te_cis1_df = X_test.iloc[:,1:6001]\n",
    "te_cis2_df = X_test.iloc[:,6001:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape,extra_shape,output_shape):\n",
    "    input_seq1 = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    input_seq2 = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    input_extra = tf.keras.layers.Input(shape=(extra_shape,))\n",
    "\n",
    "    # 시퀀스 1에 대한 처리\n",
    "    seq1_layer = tf.keras.layers.Dense(1024, activation='relu')(input_seq1)\n",
    "    seq1_layer = tf.keras.layers.Dense(512, activation='relu')(seq1_layer)\n",
    "    seq1_layer = tf.keras.layers.Dense(256, activation='relu')(seq1_layer)\n",
    "    seq1_layer = tf.keras.layers.Dense(128, activation='relu')(seq1_layer)\n",
    "\n",
    "    # 시퀀스 2에 대한 처리\n",
    "    seq2_layer = tf.keras.layers.Dense(1024, activation='relu')(input_seq2)\n",
    "    seq2_layer = tf.keras.layers.Dense(512, activation='relu')(seq2_layer)\n",
    "    seq2_layer = tf.keras.layers.Dense(256, activation='relu')(seq2_layer)\n",
    "    seq2_layer = tf.keras.layers.Dense(128, activation='relu')(seq2_layer)\n",
    "\n",
    "    # 추가 데이터에 대한 처리\n",
    "    extra_layer = tf.keras.layers.Dense(16, activation='relu')(input_extra)\n",
    "    extra_layer = tf.keras.layers.Dense(32, activation='relu')(extra_layer)\n",
    "    extra_layer = tf.keras.layers.Dense(64, activation='relu')(extra_layer)\n",
    "    extra_layer = tf.keras.layers.Dense(128, activation='relu')(extra_layer)\n",
    "\n",
    "    # 시퀀스와 추가 데이터 결합\n",
    "    combined = tf.keras.layers.Concatenate()([seq1_layer, seq2_layer, extra_layer])\n",
    "    # 결합된 시퀀스와 추가 데이터 처리\n",
    "    layer = tf.keras.layers.Dense(128, activation='relu')(combined)\n",
    "    layer = tf.keras.layers.Dense(64, activation='relu')(layer)\n",
    "    layer = tf.keras.layers.Dense(32, activation='relu')(layer)\n",
    "\n",
    "    # 출력 레이어 (9개의 클래스에 대한 확률 출력)\n",
    "    output = tf.keras.layers.Dense(output_shape, activation='relu')(layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_seq1, input_seq2, input_extra], outputs=output)\n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "model = create_model(tr_cis1_df.shape[1],tr_radius_df.shape[1],y_train.shape[1])\n",
    "\n",
    "# 모델 컴파일\n",
    "opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=opt, loss='mae',\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError(),tf.keras.metrics.RootMeanSquaredError(),\n",
    "                           tf.keras.metrics.MeanAbsoluteError(),tf.keras.metrics.MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 준비\n",
    "input_data = [tr_cis1_df, tr_cis2_df, tr_radius_df]  # 시퀀스 1, 시퀀스 2, 추가 데이터를 리스트로 묶어 입력 데이터로 사용\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(input_data, y_train, epochs=100, batch_size=32,\n",
    "          validation_split=0.2,validation_batch_size=32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_data = [X_test_cis1, X_test_cis2, test_radius_val]\n",
    "\n",
    "y_pred = model.predict(test_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X Test Length : {len(input_data)}\")\n",
    "print(f\"Y Test Length : {len(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X Test Length : {len(test_input_data)}\")\n",
    "print(f\"Y Test Length : {len(y_test)}\")\n",
    "print(f\"Y Pred Length : {len(y_pred)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df, mse_df, rmse_df, mape_df, mpe_df = evaluation.get_evalution(test_input_data, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 mae : {sum(mae_df['1000'].values)/len(mae_df['1000'])}\")\n",
    "print(f\"300 mae : {sum(mae_df['300'].values)/len(mae_df['300'])}\")\n",
    "print(f\"150 mae : {sum(mae_df['150'].values)/len(mae_df['150'])}\")\n",
    "print(f\"100 mae : {sum(mae_df['100'].values)/len(mae_df['100'])}\")\n",
    "print(f\"50 mae : {sum(mae_df['50'].values)/len(mae_df['50'])}\")\n",
    "print(f\"10 mae : {sum(mae_df['10'].values)/len(mae_df['10'])}\")\n",
    "print(f\"5 mae : {sum(mae_df['5'].values)/len(mae_df['5'])}\")\n",
    "print(f\"2 mae : {sum(mae_df['2'].values)/len(mae_df['2'])}\")\n",
    "print(f\"1 mae : {sum(mae_df['1'].values)/len(mae_df['1'])}\")\n",
    "print(f\"Total mae : {sum(mae_df['Total'].values)/len(mae_df['Total'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 mse : {sum(mse_df['1000'].values)/len(mse_df['1000'])}\")\n",
    "print(f\"300 mse : {sum(mse_df['300'].values)/len(mse_df['300'])}\")\n",
    "print(f\"150 mse : {sum(mse_df['150'].values)/len(mse_df['150'])}\")\n",
    "print(f\"100 mse : {sum(mse_df['100'].values)/len(mse_df['100'])}\")\n",
    "print(f\"50 mse : {sum(mse_df['50'].values)/len(mse_df['50'])}\")\n",
    "print(f\"10 mse : {sum(mse_df['10'].values)/len(mse_df['10'])}\")\n",
    "print(f\"5 mse : {sum(mse_df['5'].values)/len(mse_df['5'])}\")\n",
    "print(f\"2 mse : {sum(mse_df['2'].values)/len(mse_df['2'])}\")\n",
    "print(f\"1 mse : {sum(mse_df['1'].values)/len(mse_df['1'])}\")\n",
    "print(f\"Total mse : {sum(mse_df['Total'].values)/len(mse_df['Total'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"1000 rmse : {sum(rmse_df['1000'].values)/len(rmse_df['1000'])}\")\n",
    "print(f\"300 rmse : {sum(rmse_df['300'].values)/len(rmse_df['300'])}\")\n",
    "print(f\"150 rmse : {sum(rmse_df['150'].values)/len(rmse_df['150'])}\")\n",
    "print(f\"100 rmse : {sum(rmse_df['100'].values)/len(rmse_df['100'])}\")\n",
    "print(f\"50 rmse : {sum(rmse_df['50'].values)/len(rmse_df['50'])}\")\n",
    "print(f\"10 rmse : {sum(rmse_df['10'].values)/len(rmse_df['10'])}\")\n",
    "print(f\"5 rmse : {sum(rmse_df['5'].values)/len(rmse_df['5'])}\")\n",
    "print(f\"2 rmse : {sum(rmse_df['2'].values)/len(rmse_df['2'])}\")\n",
    "print(f\"1 rmse : {sum(rmse_df['1'].values)/len(rmse_df['1'])}\")\n",
    "print(f\"Total rmse : {sum(rmse_df['Total'].values)/len(rmse_df['Total'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
